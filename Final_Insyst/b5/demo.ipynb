{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 12...\n",
      "Fetching page 13...\n",
      "Fetching page 14...\n",
      "Fetching page 15...\n",
      "Fetching page 16...\n",
      "Fetching page 17...\n",
      "Fetching page 18...\n",
      "Fetching page 19...\n",
      "Total reviews fetched: 190\n",
      "Total unique reviews: 98\n",
      "Extracted 97 reviews and saved to data_test/test_1.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def get_reviews(product_id, page=1):\n",
    "    url = f\"https://tiki.vn/api/v2/reviews?product_id={product_id}&limit=10&page={page}&include=comments\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        reviews = data.get('data', [])\n",
    "        return reviews\n",
    "    else:\n",
    "        print(f\"Failed to fetch reviews: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def save_reviews_to_file(reviews, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(reviews, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def remove_duplicates(reviews):\n",
    "    # Dùng set để loại bỏ trùng lặp dựa trên nội dung bình luận\n",
    "    seen = set()\n",
    "    unique_reviews = []\n",
    "    for review in reviews:\n",
    "        content = review.get('content', '').strip()\n",
    "        if content not in seen:\n",
    "            seen.add(content)\n",
    "            unique_reviews.append(review)\n",
    "    return unique_reviews\n",
    "\n",
    "def extract_reviews_to_csv(json_file, csv_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    filtered_reviews = []\n",
    "    \n",
    "    # Lọc bình luận và nhãn\n",
    "    for review in data:\n",
    "        content = review.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if content:\n",
    "            filtered_reviews.append({\"content\": content})\n",
    "    \n",
    "    # Ghi dữ liệu vào file CSV\n",
    "    with open(csv_file, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"content\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(filtered_reviews)\n",
    "    \n",
    "    print(f\"Extracted {len(filtered_reviews)} reviews and saved to {csv_file}\")\n",
    "\n",
    "def main():\n",
    "    product_id = 154298071  # ID của sản phẩm từ URL\n",
    "    all_reviews = []\n",
    "    for page in range(1, 20):  # Crawl 9 trang\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        reviews = get_reviews(product_id, page)\n",
    "        if not reviews:\n",
    "            break\n",
    "        all_reviews.extend(reviews)\n",
    "    \n",
    "    print(f\"Total reviews fetched: {len(all_reviews)}\")\n",
    "\n",
    "    # Loại bỏ bình luận trùng lặp\n",
    "    unique_reviews = remove_duplicates(all_reviews)\n",
    "    print(f\"Total unique reviews: {len(unique_reviews)}\")\n",
    "\n",
    "    # Lưu các bình luận đã loại bỏ trùng lặp\n",
    "    save_reviews_to_file(unique_reviews, \"data_test/test_1.json\")\n",
    "\n",
    "    input_file = \"data_test/test_1.json\"  # Tệp JSON đầu vào\n",
    "    output_csv = \"data_test/test_1.csv\"  # Tệp CSV đầu ra\n",
    "    extract_reviews_to_csv(input_file, output_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu chưa được làm sạch: \n",
      "                                              content\n",
      "0  Giá cả hợp lý. Quần may kỹ, chắc chắn. Cắt chỉ...\n",
      "1  Sản phẩm quá tuyệt vời, giao hàng nhanh đóng g...\n",
      "2  Tiki giao 2h Siêu nhanh, kịp nhận quần để tham...\n",
      "3  hàng cực kì chất lượng,kiểu dáng màu sắc đẹp. ...\n",
      "4  Tiki giao hàng đúng hạn. Đóng gói đẹp, cẩn thậ...\n",
      "\n",
      "Dữ liệu đã được làm sạch: \n",
      "                                              content\n",
      "0  giá hợp lý quần may kỹ chắn cắt thừa vệ sinh s...\n",
      "1  sản phẩm tuyệt vời giao hàng đóng gói siêu cẩn...\n",
      "2  tiki giao 2h siêu kịp quần tham gia giải chạy ...\n",
      "3  hàng cực kì chất kiểu dáng màu sắc đẹp hộp gói...\n",
      "4  tiki giao hàng hạn đóng gói đẹp cẩn thận hàng ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc tệp stopword_vietnamese.txt và tạo danh sách từ dừng\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords = file.readlines()\n",
    "    stopwords = [word.strip() for word in stopwords]  # Loại bỏ khoảng trắng thừa\n",
    "    return stopwords\n",
    "\n",
    "# Tải stopwords\n",
    "stop_words = load_stopwords(\"stopwords_vietnamese.txt\")\n",
    "\n",
    "# Loại bỏ từ dừng\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Chuấn hóa dữ liệu\n",
    "def standardize_data(row):\n",
    "    # Convert to lowercase\n",
    "    row = row.lower()\n",
    "    \n",
    "    # Remove stopwords (if you have a list of stopwords, you can use it here)\n",
    "    row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n",
    "        .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
    "        .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
    "        .replace('\"', \" \").replace(\"'\", \" \") \\\n",
    "        .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
    "        .replace(\"-\", \" \").replace(\"?\", \" \") \\\n",
    "        .replace(\"(\", \" \").replace(\")\", \" \")\n",
    "\n",
    "    row = row.strip()\n",
    "    return row\n",
    "# Chưa được xử lý dữ liệu\n",
    "df = pd.read_csv(\"data_test/test_1.csv\")\n",
    "print(\"Dữ liệu chưa được làm sạch: \\n\",df.head(), end=\"\\n\\n\")\n",
    "# Đã được xử lý làm sạch dữ liệu\n",
    "df['content'] = df['content'].apply(standardize_data).apply(remove_stopwords)\n",
    "print(\"Dữ liệu đã được làm sạch: \\n\",df.head())\n",
    "\n",
    "# Lưu dữ liệu đã xử lý vào file mới\n",
    "output_file = \"data_test/processed_test_1.csv\"\n",
    "df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tải dữ liệu từ data_test/processed_test_1.csv với 97 bình luận.\n",
      "Bắt đầu gán nhãn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 4/4 [00:11<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu file với nhãn tại: data_test/output_test_labeled_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Đường dẫn file\n",
    "input_file = \"data_test/processed_test_1.csv\"  # File đầu vào\n",
    "output_file = \"data_test/output_test_labeled_1.csv\"  # File đầu ra\n",
    "\n",
    "# Khởi tạo model và tokenizer\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Đưa model về GPU nếu có\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Dataset tùy chỉnh cho batch processing\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, comments, tokenizer, max_len=128):\n",
    "        self.comments = comments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        comment = str(self.comments[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            comment,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Hàm dự đoán nhãn\n",
    "def predict_labels(comments, batch_size=32):\n",
    "    dataset = CommentDataset(comments, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    return all_preds\n",
    "\n",
    "# Đọc file đầu vào\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "    if \"content\" not in df.columns:\n",
    "        raise KeyError(\"Cột 'content' không tồn tại trong file CSV.\")\n",
    "    print(f\"Đã tải dữ liệu từ {input_file} với {len(df)} bình luận.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Không thể đọc file CSV: {e}\")\n",
    "\n",
    "# Dự đoán nhãn\n",
    "try:\n",
    "    comments = df[\"content\"].fillna(\"\").tolist()  # Thay thế giá trị null bằng chuỗi rỗng\n",
    "    print(\"Bắt đầu gán nhãn...\")\n",
    "    labels = predict_labels(comments)\n",
    "    df[\"label\"] = labels\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Lỗi khi dự đoán nhãn: {e}\")\n",
    "\n",
    "# Lưu kết quả ra file\n",
    "try:\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Đã lưu file với nhãn tại: {output_file}\")\n",
    "except Exception as e:\n",
    "    raise IOError(f\"Lỗi khi lưu file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "def load_pretrainModel(data):\n",
    "    \n",
    "    '''\n",
    "    Load pretrain model/ tokenizers\n",
    "    Return : features\n",
    "    '''\n",
    "    MODEL_NAME = 'bert-base-uncased'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    #encode lines\n",
    "    tokenized = data.apply((lambda x: tokenizer.encode(x, add_special_tokens = True)))\n",
    "\n",
    "    # get lenght max of tokenized\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    print('max len:', max_len)\n",
    "\n",
    "    # if lenght of tokenized not equal max_len , so padding value 0\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    print('padded:', padded[1])\n",
    "    print('len padded:', padded.shape)\n",
    "\n",
    "    #get attention mask ( 0: not has word, 1: has word)\n",
    "    attention_mask = np.where(padded ==0, 0,1)\n",
    "    print('attention mask:', attention_mask[1])\n",
    "\n",
    "    # Convert input to tensor\n",
    "    padded = torch.tensor(padded)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "\n",
    "    # Load model\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(padded, attention_mask =attention_mask)\n",
    "    #     print('last hidden states:', last_hidden_states)\n",
    "\n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    print('features:', features)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len: 64\n",
      "padded: [  101  2624  6887  3286 10722  6672  2102 29536  2072 27699  2080  6865\n",
      "  1102  5063  2175  2072  9033 13765  2064  2084  6865  8945  2278 11382\n",
      " 14163  2050  6865 14841  3211  3393  1102  4887 15030  1037  1062 23564\n",
      "  2100  1056  5705  4497   102     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "len padded: (97, 64)\n",
      "attention mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(analyze(result))\n\u001b[1;32m---> 16\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_test/output_test_labeled_1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m      7\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace NaN with empty string\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Load weights\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m, in \u001b[0;36mload_pretrainModel\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     42\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m model(padded, attention_mask \u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#     print('last hidden states:', last_hidden_states)\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mlast_hidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures:\u001b[39m\u001b[38;5;124m'\u001b[39m, features)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "def predict(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['content'] = data['content'].fillna('')  # Replace NaN with empty string\n",
    "    features = load_pretrainModel(data['content'])\n",
    "    # 2. Load weights\n",
    "    model = joblib.load('save_model.pkl')\n",
    "    # 3. Result\n",
    "    result = model.predict(features)\n",
    "    print(result)\n",
    "    print(analyze(result))\n",
    "\n",
    "predict(\"data_test/output_test_labeled_1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
