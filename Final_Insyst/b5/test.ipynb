{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup # parse html\n",
    "import re #regex\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import joblib #load, dump pkl\n",
    "from underthesea import word_tokenize #word_tokenize of lines\n",
    "import numpy as np\n",
    "import transformers as ppb # load model BERT\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "# scrap comment = selenium\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "# import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craw comment of product tiki, lazada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "def load_url_selenium_lazada(url):\n",
    "    # Create a Service object to specify the path to ChromeDriver\n",
    "    service = Service(executable_path='chromedriver-win64/chromedriver.exe')\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    \n",
    "    print(\"Loading url=\", url)\n",
    "    driver.get(url)\n",
    "    list_review = []\n",
    "    # just crawl 10 pages\n",
    "    x = 0\n",
    "    while x < 10:\n",
    "        try:\n",
    "            # Wait for reviews to load\n",
    "            WebDriverWait(driver, 5).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \"div.item\")))\n",
    "        except:\n",
    "            print('No comments found')\n",
    "            break\n",
    "\n",
    "        product_reviews = driver.find_elements(By.CSS_SELECTOR, \"[class='item']\")\n",
    "        # Get product reviews\n",
    "        for product in product_reviews:\n",
    "            review = product.find_element(By.CSS_SELECTOR, \"[class='content']\").text\n",
    "            if review.strip():\n",
    "                print(review, \"\\n\")\n",
    "                list_review.append(review)\n",
    "        \n",
    "        # Check if the next button is disabled or not\n",
    "        if len(driver.find_elements(By.CSS_SELECTOR, \"button.next-pagination-item.next[disabled]\")) > 0:\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                button_next = WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"button.next-pagination-item.next\")))\n",
    "                driver.execute_script(\"arguments[0].click();\", button_next)\n",
    "                print(\"Next page\")\n",
    "                time.sleep(2)\n",
    "                x += 1\n",
    "            except TimeoutException:\n",
    "                print('Next button not found or timeout')\n",
    "                break\n",
    "                \n",
    "    driver.close()\n",
    "    return list_review\n",
    "\n",
    "def load_url_selenium_tiki(url):\n",
    "    # Create a Service object to specify the path to ChromeDriver\n",
    "    service = Service(executable_path='chromedriver-win64/chromedriver.exe')\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    \n",
    "    print(\"Loading url=\", url)\n",
    "    driver.get(url)\n",
    "    list_review = []\n",
    "    # just crawl 10 pages\n",
    "    x = 0\n",
    "    while x < 10:\n",
    "        try:\n",
    "            # Wait for reviews to load\n",
    "            WebDriverWait(driver, 5).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \"div.review-comment\")))\n",
    "        except:\n",
    "            print('No comments found')\n",
    "            break\n",
    "\n",
    "        product_reviews = driver.find_elements(By.CSS_SELECTOR, \"[class='review-comment']\")\n",
    "        # Get product reviews\n",
    "        for product in product_reviews:\n",
    "            review = product.find_element(By.CSS_SELECTOR, \"[class='review-comment__content']\").text\n",
    "            if review.strip():\n",
    "                print(review, \"\\n\")\n",
    "                list_review.append(review)\n",
    "        \n",
    "        # Check if the next button is disabled or not\n",
    "        try:\n",
    "            button_next = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"[class = 'btn next']\")))\n",
    "            driver.execute_script(\"arguments[0].click();\", button_next)\n",
    "            print(\"Next page\")\n",
    "            time.sleep(2)\n",
    "            x += 1\n",
    "        except (TimeoutException, WebDriverException) as e:\n",
    "            print('Failed to load next page:', e)\n",
    "            break\n",
    "                \n",
    "    driver.close()\n",
    "    return list_review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard data, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardize_data(row):\n",
    "    # remove stopword\n",
    "    # Remove . ? , at index final\n",
    "    row = re.sub(r\"[\\.,\\?]+$-\", \"\", row)\n",
    "    # Remove all . , \" ... in sentences\n",
    "    row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n",
    "        .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
    "        .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
    "        .replace('\"', \" \").replace(\"'\", \" \") \\\n",
    "        .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
    "        .replace(\"-\", \" \").replace(\"?\", \" \")\n",
    "\n",
    "    row = row.strip()\n",
    "    return row\n",
    "\n",
    "# Tokenizer\n",
    "def tokenizer(row):\n",
    "    return word_tokenize(row, format=\"text\")\n",
    "\n",
    "def analyze(result):\n",
    "    bad = np.count_nonzero(result)\n",
    "    good = len(result) - bad\n",
    "    print(\"No of bad and neutral comments = \", bad)\n",
    "    print(\"No of good comments = \", good)\n",
    "\n",
    "    if good>bad:\n",
    "        return \"Good! You can buy it!\"\n",
    "    else:\n",
    "        return \"Bad! Please check it carefully!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(data):\n",
    "    # 1. Standardize data\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    print('data frame:', data_frame)\n",
    "    data_frame[0] = data_frame[0].apply(standardize_data)\n",
    "\n",
    "    # 2. Tokenizer\n",
    "    data_frame[0] = data_frame[0].apply(tokenizer)\n",
    "\n",
    "    # 3. Embedding\n",
    "    X_val = data_frame[0]\n",
    "    return X_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrainModel(data):\n",
    "    \n",
    "    '''\n",
    "    Load pretrain model/ tokenizers\n",
    "    Return : features\n",
    "    '''\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    #encode lines\n",
    "    tokenized = data.apply((lambda x: tokenizer.encode(x, add_special_tokens = True)))\n",
    "\n",
    "    # get lenght max of tokenized\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    print('max len:', max_len)\n",
    "\n",
    "    # if lenght of tokenized not equal max_len , so padding value 0\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    print('padded:', padded[1])\n",
    "    print('len padded:', padded.shape)\n",
    "\n",
    "    #get attention mask ( 0: not has word, 1: has word)\n",
    "    attention_mask = np.where(padded ==0, 0,1)\n",
    "    print('attention mask:', attention_mask[1])\n",
    "\n",
    "    # Convert input to tensor\n",
    "    padded = torch.tensor(padded)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "\n",
    "    # Load model\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(padded, attention_mask =attention_mask)\n",
    "    #     print('last hidden states:', last_hidden_states)\n",
    "\n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    print('features:', features)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading url= https://tiki.vn/dien-thoai-samsung-galaxy-a05s-4gb-128gb-da-kich-hoat-bao-hanh-dien-tu-hang-chinh-hang-p273258825.html\n",
      "No comments found\n",
      "data frame: Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(analyze(result))\n\u001b[1;32m---> 15\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://tiki.vn/dien-thoai-samsung-galaxy-a05s-4gb-128gb-da-kich-hoat-bao-hanh-dien-tu-hang-chinh-hang-p273258825.html\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#data = load_url_selenium_lazada(url)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m load_url_selenium_tiki(url)\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mprocessing_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m features \u001b[38;5;241m=\u001b[39m load_pretrainModel(data)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Load weights\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mprocessing_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m data_frame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata frame:\u001b[39m\u001b[38;5;124m'\u001b[39m, data_frame)\n\u001b[1;32m----> 5\u001b[0m data_frame[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_frame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(standardize_data)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 2. Tokenizer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m data_frame[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m data_frame[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(tokenizer)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def predict(url):\n",
    "    # 1. Load URL and print comments\n",
    "    if url== \"\":\n",
    "        url = \"https://tiki.vn/dien-thoai-samsung-galaxy-m31-128gb-6gb-hang-chinh-hang-p58259141.html\"\n",
    "    #data = load_url_selenium_lazada(url)\n",
    "    data = load_url_selenium_tiki(url)\n",
    "    data = processing_data(data)\n",
    "    features = load_pretrainModel(data)\n",
    "    # 2. Load weights\n",
    "    model = joblib.load('save_model.pkl')\n",
    "    # 3. Result\n",
    "    result = model.predict(features)\n",
    "    print(result)\n",
    "    print(analyze(result))\n",
    "predict(url ='https://tiki.vn/dien-thoai-samsung-galaxy-a05s-4gb-128gb-da-kich-hoat-bao-hanh-dien-tu-hang-chinh-hang-p273258825.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
