{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 1)\n",
      "0      18/4 Lazada giao nhưng 19/4 tôi mới khui hộp m...\n",
      "1      Đã nhận được hàng, máy đẹp, giao hàng nhanh. C...\n",
      "2      Tôi mua sản phẩm samsung Galaxy M10. Máy bị lỗ...\n",
      "3      Đặt Samsung Galaxy M10,giao hàng ngoài hộp M10...\n",
      "4      Hàng đẹp xài ngon so với giá tiền, nên mua cho...\n",
      "                             ...                        \n",
      "128               Mua 25/2 mở ra thấy bị lỗi sơn viền ,1\n",
      "129       Hàng ok nhưng thấy vỏ hộp có vết mực bi đánh,1\n",
      "130                Sản_phẩm đúng như mô_tả Nguyên seal,0\n",
      "131    Mọi thứ quay mồng mồng mà không làm được gì. G...\n",
      "132    mới mua về mà có cuộc gọi đến đều không bấm đư...\n",
      "Name: 0, Length: 133, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_crawler.csv', delimiter='\\t', header=None)\n",
    "print(df.shape)\n",
    "# get all rows\n",
    "print(df[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode [101, 1102, 2050, 18699, 2319, 1102, 19098, 2278, 6865, 1010, 2089, 1102, 13699, 1010, 27699, 2080, 6865, 18699, 2319, 2232, 1012, 11503, 2006, 14841, 3211, 1010, 1014, 102]\n",
      "decode [CLS] đa nhan đuoc hang, may đep, giao hang nhanh. cam on tiki, 0 [SEP]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load pretrain model/ tokenizers\n",
    "'''\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#encode lines\n",
    "tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens = True)))\n",
    "print('encode',tokenized[1])\n",
    "# decode\n",
    "print('decode',tokenizer.decode(tokenized[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning model and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: (133,)\n",
      "max len: 138\n",
      "padded: [  101  1102  2050 18699  2319  1102 19098  2278  6865  1010  2089  1102\n",
      " 13699  1010 27699  2080  6865 18699  2319  2232  1012 11503  2006 14841\n",
      "  3211  1010  1014   102     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "len padded: (133, 138)\n",
      "attention mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "features: [[-0.50565875  0.36628267  0.37790143 ...  0.28829202  0.22329322\n",
      "   0.9688276 ]\n",
      " [-0.6813469  -0.2824761   0.16296086 ...  0.25529566  0.13253303\n",
      "   0.97691643]\n",
      " [-0.71128637  0.30791706  0.6924364  ...  0.37146547  0.6316519\n",
      "   0.7060471 ]\n",
      " ...\n",
      " [-0.680228   -0.06411599 -0.2178851  ... -0.0400923   0.23876162\n",
      "   0.6842745 ]\n",
      " [-0.95865506 -0.01374028  0.36309147 ... -0.07484522  0.4939244\n",
      "   1.1382143 ]\n",
      " [-0.779383    0.11393505 -0.01372401 ...  0.06957778  0.20980455\n",
      "   0.776577  ]]\n",
      "score: 1.0\n"
     ]
    }
   ],
   "source": [
    "#get all label \n",
    "labels = np.zeros(len(df[0]))\n",
    "for i in range(len(df[0])):\n",
    "    labels[i] = df[0][i][-1]\n",
    "print('labels shape:', labels.shape)\n",
    "\n",
    "# get lenght max of tokenized\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print('max len:', max_len)\n",
    "\n",
    "# if lenght of tokenized not equal max_len , so padding value 0\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "print('padded:', padded[1])\n",
    "print('len padded:', padded.shape)\n",
    "\n",
    "#get attention mask ( 0: not has word, 1: has word)\n",
    "attention_mask = np.where(padded ==0, 0,1)\n",
    "print('attention mask:', attention_mask[1])\n",
    "\n",
    "# Convert input to tensor\n",
    "padded = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "\n",
    "# Train model\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(padded, attention_mask =attention_mask)\n",
    "#     print('last hidden states:', last_hidden_states)\n",
    "\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "print('features:', features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "\n",
    "cl = LogisticRegression()\n",
    "cl.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(cl, 'save_model.pkl')\n",
    "sc = cl.score(X_test, y_test)\n",
    "print('score:', sc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
