{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('reviews.csv', delimiter='\\t', header=None)\n",
    "print(df.shape)\n",
    "# get all rows\n",
    "# print(df[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved with BERT features: reviews_with_features_bert.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode [101, 22794, 2100, 6865, 25175, 15775, 2100, 2006, 21360, 18765, 16215, 2072, 14684, 2050, 12170, 3388, 3841, 1047, 19991, 18699, 5575, 25945, 7658, 2232, 1016, 15125, 11265, 2078, 18371, 17214, 1010, 1102, 19098, 2278, 9745, 9543, 2232, 4830, 2226, 19438, 14163, 2072, 9212, 2232, 2139, 9610, 2226, 1010, 17214, 27793, 2072, 1102, 2319, 2232, 27699, 1019, 1008, 102]\n",
      "decode [CLS] thay hang moi chay on lau dai thi chua biet ben khong nhung bao hanh 2 nam nen yen tam, đuoc tang tinh dau thom mui chanh de chiu, tam thoi đanh gia 5 * [SEP]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load pretrain model/ tokenizers\n",
    "'''\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#encode lines\n",
    "tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens = True)))\n",
    "print('encode',tokenized[1])\n",
    "# decode\n",
    "print('decode',tokenizer.decode(tokenized[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning model and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m0\u001b[39m][i][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# get lenght max of tokenized\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '.'"
     ]
    }
   ],
   "source": [
    "#get all label \n",
    "labels = np.zeros(len(df[0]))\n",
    "for i in range(len(df[0])):\n",
    "    labels[i] = df[0][i][-1]\n",
    "print('labels shape:', labels.shape)\n",
    "\n",
    "# get lenght max of tokenized\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print('max len:', max_len)\n",
    "\n",
    "# if lenght of tokenized not equal max_len , so padding value 0\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "print('padded:', padded[1])\n",
    "print('len padded:', padded.shape)\n",
    "\n",
    "#get attention mask ( 0: not has word, 1: has word)\n",
    "attention_mask = np.where(padded ==0, 0,1)\n",
    "print('attention mask:', attention_mask[1])\n",
    "\n",
    "# Convert input to tensor\n",
    "padded = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "\n",
    "# Train model\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(padded, attention_mask =attention_mask)\n",
    "#     print('last hidden states:', last_hidden_states)\n",
    "\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "print('features:', features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "\n",
    "cl = LogisticRegression()\n",
    "cl.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(cl, 'save_model.pkl')\n",
    "sc = cl.score(X_test, y_test)\n",
    "print('score:', sc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
